# -*- coding: utf-8 -*-
"""Linear Regression with python and scikit-learn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RuzxjVKJpgz5V8eunpdbQ1R1fgYRRlRR
"""

medical_charges_url = "https://github.com/stedy/Machine-Learning-with-R-datasets"
from urllib.request import urlretrieve
urlretrieve(medical_charges_url,"medical.csv")



!pip install pandas --quiet

import pandas as pd

df = pd.read_csv("medical.csv", delimiter='\t')
df = pd.read_csv("medical.csv", error_bad_lines=False)

df.head()

import pandas as pd

# URL of the raw dataset file on GitHub
url = "https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv"

# Read the dataset into a Pandas DataFrame
df = pd.read_csv(url)

# Display the first few rows of the DataFrame
df.head()

df

df.describe()

df.info()

df.age.describe()

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np

sns.set_style("darkgrid")
plt.rcParams["font.size"]=14
plt.rcParams["figure.figsize"]=(10,6)

df.age.describe()

"""Given the age's minimum is 18 and the maximum is 64 so the total bin's possible is 64-18 = 46 + 1 = 47"""

fig = px.histogram(df,x="age",nbins=47,marginal="box",title="Distribution of Age")
fig.update_layout(bargap=0.2)
fig.show()

fig = px.histogram(df,x="age",nbins=47,title="Distribution of Age")
fig.update_layout(bargap=0.1)
fig.show()

"""TO change the color in histogram we want to use : color_discrete_sequence=["red"]

#To visualize the BMI using Histogram
"""

df.bmi.describe()

fig = px.histogram(df,x="bmi",color_discrete_sequence=["red"],nbins=39)
fig.update_layout(bargap=0.2)
fig.show()

"""To visualize the smoker using Histogram"""

fig=px.histogram(df,x="charges",color="smoker",marginal="box",color_discrete_sequence=["green","grey"],title="Annual Medical Charges")
fig.update_layout(bargap=0.2)
fig.show()

fig = px.histogram(df,x="smoker",color="sex",color_discrete_sequence=["black","red"],title="smoker")
fig.show()

df

df.children.describe()

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

sns.set_style("darkgrid")
plt.rcParams["figure.figsize"]=(10,5)

fig = px.histogram(df,x="children",color_discrete_sequence=["orange"],nbins=6)
fig.update_layout(bargap=0.2)

"""# Now we are jumping into ScatterPlot

We are looking for the relationship betweeen age and charges.
"""

px.scatter(df,x="age",y="charges",title="Age vs Charges",hover_data=["sex"],color="smoker",opacity=0.8)

"""#Now we are going to calculate the corelation's of the columns

Note : The correlation can't be higher than 1 and lower than -1

The relation between age and charges
"""

df

df.charges.corr(df.age)

df.bmi.corr(df.charges)

df.charges.corr(df.children)

df.smoker

smoker_values = {"no" : 0 , "yes" : 1}
smoker_numeric = df.smoker.map(smoker_values)

smoker_numeric

df.charges.corr(smoker_numeric)

"""Overall correlations"""

df.corr()

"""Correlation is basically visualized in the heatmap"""

sns.heatmap(df.corr(),cmap="Reds",annot=True)
plt.title("Correlation Matrix")

df

non_smoker_df = df[df.smoker == "no"]

non_smoker_df

px.scatter(non_smoker_df,x="age",y="charges",)



"""#Now we are moving to LinearRegression Model :"""

#y = charges
#x = age
def estimated_charges(age,w,b):
  return(w*age+b)

w=50
b=100
age=30
ages = non_smoker_df.age
estimated_charges = estimated_charges(ages,w,b)
estimated_charges

non_smoker_df.charges

plt.plot(ages,estimated_charges)
plt.xlabel("Age")
plt.ylabel("Estimated_Charges")

#Now we are checking how accurate our model is :

fig1 = px.line(x=ages,y=estimated_charges,color_discrete_sequence=["red"])
fig1.add_trace(px.scatter(x=ages,y=non_smoker_df.charges).data[0])
fig1.update_layout(xaxis_title="ages",yaxis_title="charges")
fig1.show()

fig2 = px.line(x=non_smoker_df.age,y=estimated_charges,color_discrete_sequence=["red"])
fig2.add_trace(px.scatter(x=non_smoker_df.age,y=non_smoker_df.charges).data[0])
fig2.update_layout(xaxis_title="ages",yaxis_title="charges")
fig2.show()

def estimated_charges(ages,w,b):
  return w*ages+b

def sample_parameter(w,b):
  ages=non_smoker_df.age
  target=non_smoker_df.charges

  estimated_charges_values = estimated_charges(ages,w,b)

  fig1 = px.line(x=ages,y=estimated_charges_values,color_discrete_sequence=["red"])
  fig1.add_trace(px.scatter(x=ages,y=target).data[0])
  fig1.update_layout(xaxis_title="ages",yaxis_title="Charges",title="Estimated vs Actual Values")
  fig1.show()

sample_parameter(200,1000)

import numpy as np

"""RMSE"""

predicted = estimated_charges
actual = non_smoker_df.charges
def rmse(actual,predicted):
  return np.sqrt(np.mean(np.square(actual-predicted)))

sample_parameter(200,1000)

actual = non_smoker_df.charges
predicted = estimated_charges(ages,200,1000)

result = rmse(actual,predicted)
print(result)

actual = non_smoker_df.charges
predicted = estimated_charges(ages,190,1000)

result = rmse(actual,predicted)
print(result)



"""#Linear Regression with SCIKIT-LEARN"""

from sklearn.linear_model import LinearRegression

"""creating a new model object"""

model = LinearRegression()

"""Now , we want to use fit method of the model to find the best fit line for the input and targets"""

help(model.fit)

"""# This is 1 dimensional but we need 2 dimensiona to fit into the input data to linear Regression"""

inputs = non_smoker_df.age
print(inputs.shape)
print(inputs)

"""for that we want to use non_smoker_df[["age"]] and for targets we need only one dimensional"""

inputs = non_smoker_df[["age"]]
targets = non_smoker_df.charges
print(inputs.shape)
print(inputs)

"""Now we can fit the model"""

model.fit(inputs,targets)
# we can use in1 line : model = LinearRegression().fit(inputs,targets)

"""Now we can predict the output for the input"""

model.predict(np.array([[13],[28],[57]]))

predictions = model.predict(inputs)
predictions

inputs

targets

rmse(targets,predictions)

"""Now we can check what is the value of w and b which is used by Model"""

#w
model.coef_

#b
model.intercept_

sample_parameter(model.coef_,model.intercept_)

"""Nxt by using the SGDRegressor"""

from sklearn.linear_model import SGDRegressor
model = SGDRegressor()
model.fit(inputs,targets)
model.predict(inputs)

targets

rmse(model.coef_,model.intercept_)

"""compared to SGD Regressor , the linear regression model performs the best

#Note : Machine learning has 3 components which is model , cost Function , Optimizer

Now we are going to do with Multiple Linear Regression
"""

df

non_smoker_df

"""We have done predictions using multiple input variables such as with age , bmi , children"""

inputs,targets = non_smoker_df[["age","bmi","children"]] , non_smoker_df["charges"]
from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(inputs,targets)
predictions = model.predict(inputs)
loss = rmse(targets,predictions)
print("The loss for this model is : ",loss)

model.coef_

model.intercept_

fig = px.violin(non_smoker_df,x="children",y="charges",title="Children vs charges")
fig.show()

fig = px.strip(non_smoker_df,x="children",y="charges",title="Children vs charges")
fig.show()

fig = px.histogram(non_smoker_df,x="children",nbins=6,color_discrete_sequence=["red"])
fig.update_layout(bargap=0.2)
fig.show()



"""#In earlier we was just computing the values for the numeric columns but for prediction we need also the categorical columns to predict

1.If a categorical column has 2 categories i.e binary categories , then we can replace their values with 0 and 1

2.If a categorical column has more than 2 categories then we want to perform one-hot encoding i.e create a new column for each category with 1s and 0s

3.If the categories have a natural order(eg : cold,neutral,warm,hot) then they can be converted to numbers(eg : 1,2,3,4) These are called ordinals

#Binary Categories
"""

sns.barplot(data=df,x="smoker",y="charges")

"""Now , we are going to convert the smoker's categorical column to numerical categorical column"""

smoker_code = {"no" : 0 , "yes" : 1}
 df["smoker_code"] = df.smoker.map(smoker_code)

df.charges.corr(df.smoker_code)

inputs,targets = df[["age","bmi","children","smoker_code"]] ,df["charges"]
from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(inputs,targets)
predictions = model.predict(inputs)
loss = rmse(targets,predictions)
print("The loss for this model is : ",loss)

"""now we are going to add sex column"""

sns.barplot(data=df,x="sex",y="charges")

sex_code = {"female" : 0 , "male" : 1}
df["sex_code"] = df.sex.map(sex_code)
df

"""Now we can check the correlation for the sex column"""

df.charges.corr(df.sex_code)

"""Now we can calculate the prediction"""

inputs,targets = df[["age","bmi","children","sex_code","smoker_code"]] ,df["charges"]
from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(inputs,targets)
predictions = model.predict(inputs)
loss = rmse(targets,predictions)
print("The loss for this model is : ",loss)

"""So we noted that there is no big difference while adding the sex column so it's useless for the prediction

Now we can check the column region whether it will help in prediction , for that region is categorical so we need to use the method oneHotEncoder
"""

sns.barplot(df,x="region",y="charges")

from sklearn import preprocessing
enc = preprocessing.OneHotEncoder()
enc.fit(df[["region"]])

enc.categories_

one_hot = enc.transform(df[["region"]]).toarray()

one_hot

df[["northeast","northwest","southeast","southwest"]] = one_hot

df

input_col = df[["age","bmi","children","sex_code","smoker_code","northeast","northwest","southeast","southwest"]]
inputs,targets = input_col , df["charges"]
from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(inputs,targets)
predictions = model.predict(inputs)
loss = rmse(targets,predictions)
print("The loss for this model is : ",loss)

model.coef_

model.intercept_

weights_df = pd.DataFrame({"feature" : np.append(input_col),"weight" : np.append(model.coef_ )})

weights_df = pd.DataFrame({"feature": np.append(input_col, 1), "weight": np.append(model.coef_, model.intercept_)})

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(inputs,targets)

from sklearn import preprocessing
enc = preprocessing.OneHotEncoder()
enc.fit(df[["region"]])
one_Hot = enc.transform(df[["region"]]).toarray()

from sklearn.preprocessing import StandardScaler
numeric_col = ["age","bmi","children"]
scaler = StandardScaler()
scaler.fit(df[numeric_col])

scaler.mean_

scaler.var_

"""#Now we are going to Scale the inputs :"""

scaled_inputs = scaler.transform(df[numeric_col])
scaled_inputs

cat_col = ["smoker_code","sex_code","northeast","northwest","southeast","southwest"]
cat_data = df[cat_col].values

inputs = np.concatenate((scaled_inputs,cat_data),axis=1)
targets = df.charges
model = LinearRegression().fit(inputs,targets)
predictions = model.predict(inputs)
loss = rmse(targets,predictions)
print("The loss is : ",loss)

weights_df = pd.DataFrame({"feature": np.append(input_col, 1), "weight": np.append(model.coef_, model.intercept_)})

"""From this we can understand that smoker , age , bmi are the most important feature"""

