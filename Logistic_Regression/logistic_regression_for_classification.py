# -*- coding: utf-8 -*-
"""Logistic regression for classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t0AhxmiLgDDBJxTxoJo4M0t2oqB2czs-

It is used when our dependent variable is dichotomous or binary. It just means a variable that has only 2 outputs

For example

1. whether today it will rain or not
2. Is person having cancer or not
3. Whether a person can pay a loan or not by looking to the factors such as age,income,loan amount , no.of children etc
4. A person will survive this accident or not
5. The student will pass this exam or not.
"""



!pip install opendatasets --upgrade --quiet

import opendatasets as od

od.version()

dataset_url = 'https://www.kaggle.com/jsphyg/weather-dataset-rattle-package'

od.download(dataset_url)

import os

data_dir = "./weather-dataset-rattle-package/"

os.listdir(data_dir)

train_csv = data_dir + "./weatherAUS.csv"

train_csv

!pip install pandas --quiet

import pandas as pd
raw_df = pd.read_csv(train_csv)

raw_df

raw_df.info()

df = raw_df

df

"""# First of all we need to look for the important column which helps to give the required output , and if there is any null value in that . We need to remove the null rows in specified column"""

df.dropna(subset=["RainToday","RainTomorrow"],inplace=True)

df

df.info()

import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("darkgrid")
plt.rcParams["font.size" ]=14
plt.rcParams["figure.figsize"]=(10,6)

df

"""Now , we are going to analyze a single column which is Location in the basis of which at the exact location it had rained or not"""

fig = px.histogram(df,x="Location",color="RainToday")
fig.update_layout(bargap = 0.2)
fig.show()

df.Location.nunique()

df.Location.value_counts()

"""We can look , how the temperature look like on 03 pm"""

px.histogram(df,x="Temp3pm",color="RainTomorrow",title = "Temperature at 3pm vs Rain Tomorrow")

"""What is the highest temperature"""

df["Temp3pm"].max()

df

filter = df[df["RainToday"] == "Yes"]
filter[filter["MaxTemp"] == filter["MaxTemp"].max()]

(df["MaxTemp"]).max()

"""Now we can look into the count of Rain Tomorrow and it's actually Rained on that day or not"""

px.histogram(df,x="RainTomorrow",color="RainToday")

"""We can check whether there is relationship between MaxTemp and Mintemp to predict it will rain tomorrow or not"""

px.scatter(df,x="MinTemp",y="MaxTemp",color="RainToday")

df

px.scatter(df,x="Temp3pm",y="Humidity3pm",hover_data=["Pressure3pm","Cloud3pm"],color="RainTomorrow")

df

Rain_df = df[(df["RainTomorrow"] == "Yes") & (df["RainToday"]=="Yes")]
Rain_df

px.scatter(Rain_df,x="Temp3pm",y="Humidity3pm",hover_data=["Pressure3pm","Cloud3pm"])

Rain_df["numeric_RainToday"] = Rain_df["RainToday"].map({"Yes" : 1 , "No" : 0})

Rain_df

Rain_df.numeric_RainToday.corr(Rain_df.Temp3pm)

df["RainToday_numeric"] = df["RainToday"].map({"Yes" : 1, "No" : 0})
df

"""We found that there is less correlation between Temperature aat 3 pm and RainToday"""

df.RainToday_numeric.corr(df.Temp3pm)

df["RainTomorrow_numeric"] = df["RainTomorrow"].map({"Yes" : 1, "No" : 0})
df

filter_rain_t = df["RainToday"] == "Yes"

filter_rain_t

raw_df

df.Rainfall.corr(df.RainTomorrow_numeric)

df.Temp3pm.corr(df.RainTomorrow_numeric)

df.MinTemp.corr(df.RainTomorrow_numeric)

fig1 = sns.heatmap(df.corr(),cmap="Reds",annot = True,)

sample_fraction = 0.1
df = df.sample(frac=sample_fraction).copy()

df

raw_df



"""#We are going to split the dataset into training,validation,test

As a rule of thumb we can use around 60% of data for training set , 20% for the validation set and 20% for the test_set
"""



from sklearn.model_selection import train_test_split

"""First of all we are taking 20% of data and putting into test_df and the remaining we are putting into train_val_df"""

train_val_df , test_df = train_test_split(df,test_size=0.2,random_state=42)
train_df , val_df = train_test_split(train_val_df,test_size=0.25,random_state=42)

print("train_df.shape",train_df.shape)
print("test_df.shape",test_df.shape)
print("val_df.shape",val_df.shape)

df

plt.title("No of Rows per year")
sns.countplot(x=pd.to_datetime(df.Date).dt.year)

df = raw_df

"""While working with dates , it's often a better idea to seperate the training , validation ,test sets with time , so that the model is trained on data from the pastand evaluated on data from the future

for the current dataset , we can use the Date column in the datset to create another column for year . We'll pick the last two years for the test set , we'll pick the last two years for the test set and one year before it for the validation set
"""

df["year"] = pd.to_datetime(df["Date"]).dt.year

df

fig = px.histogram(df,x="year")
fig.update_layout(bargap=0.2)
fig.show()

"""It is always better to take last fraction as the test data , and before that as a validation data"""

train_df = df[df["year"]<2015]
val_df = df[df["year"]==2015]
test_df = df[df["year"]>2015]

print("train_df.shape",train_df.shape)
print("vaidation_df.shape",val_df.shape)
print("test_df.shape",test_df.shape)

"""While not a perfect 60-20-20 split , we have ensured that the test validation and test sets both contain data for all 12 months of the year

Identifying Input and Target Columns
"""

train_df

train_df.drop(["RainToday_numeric","RainTomorrow_numeric","year"],axis=1,inplace=True)

input_cols = list(train_df.columns)[1:-1]
target_col = "RainTomorrow"

input_cols



val_df.drop(["RainToday_numeric","RainTomorrow_numeric","year"],axis=1,inplace=True)
test_df.drop(["RainToday_numeric","RainTomorrow_numeric","year"],axis=1,inplace=True)

train_inputs = train_df[input_cols]
train_targets = train_df[target_col]
val_inputs = val_df[input_cols]
val_targets = val_df[target_col]
test_inputs = test_df[input_cols]
test_targets = test_df[target_col]

train_inputs

train_targets

numeric_col_train_inputs = train_inputs[dtype="object"]

numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()
categorical_cols = train_inputs.select_dtypes("object").columns.tolist()

numeric_cols

categorical_cols

train_inputs[numeric_cols].describe()

train_inputs[categorical_cols].describe()

"""#Imputing Missing Numeric Data"""

from sklearn.impute import SimpleImputer

"""We are going to replace the missing values with the average value in the column using the SimpleImputer class from sklearn.impute"""

imputer = SimpleImputer(strategy = "mean")

"""checking which numeric column contains null values"""

df[numeric_cols].isna().sum()

train_inputs[numeric_cols].isna().sum()

imputer.fit(df[numeric_cols])

"""After calling fit , the computed statistic for each column is stored in the statistics_ property of imputer"""

x = list(imputer.statistics_)

numeric_cols

mapping = dict(zip(numeric_cols,x))

mapping

imputer.transform(train_inputs[numeric_cols])

"""This is just an numpy array
Now , we are going to overwrite this :
"""

train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])

train_inputs

train_inputs[numeric_cols].isna().sum()

"""#Scaling Numeric Features

A good practice is to scale numeric features to a small range of values(0,1) or (-1,1)

Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss.

Optimization algo also work better in practice with smaller numbers
"""

df[numeric_cols].describe()

"""Let's use MinMaxScaler from sklearn.preprocessing to scale values to the (0,1) range."""

from sklearn.preprocessing import MinMaxScaler

?MinMaxScaler

scaler = MinMaxScaler()

"""First , we fit the scaler to the data i.e compute the range of values for each numeric column"""

scaler.fit(df[numeric_cols])

"""Now we can inspect min and max values in each column"""

print("Minimum")
list(scaler.data_min_)

print("Maximum")
list(scaler.data_max_)

train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])
val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])
test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])

train_inputs[numeric_cols].describe()

df[categorical_cols].describe()

"""# Noe we are going to convert categorical value into the binary values such as (0/1) inorder to predict"""

from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(sparse = False,handle_unknown="ignore")
enc.fit(df[categorical_cols])

enc.categories_

encoded_cols = list(enc.get_feature_names_out(categorical_cols))
encoded_cols

train_inputs[encoded_cols] = enc.transform(train_inputs[categorical_cols])
val_inputs[encoded_cols] = enc.transform(val_inputs[categorical_cols])
test_inputs[encoded_cols] = enc.transform(test_inputs[categorical_cols])

train_inputs

"""#Training Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver="liblinear")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.fit(train_inputs[numeric_cols + encoded_cols],train_targets)

"""let's check what is the weight and bias that is assigned

"""

train_inputs[numeric_cols + encoded_cols]
model.coef_

model.intercept_

x = pd.DataFrame({
    "feature" : (numeric_cols + encoded_cols),
    "weight" : model.coef_[0]
})

maxi = x["weight"].max()
x[x["weight"] == maxi]

x_sorted = x.sort_values(by="weight", ascending=False)
x_sorted

"""#Making Predictions and Evaluati ng the Model"""

x_train = train_inputs[numeric_cols + encoded_cols]
x_val = val_inputs[numeric_cols + encoded_cols]
x_test = test_inputs[numeric_cols + encoded_cols]

train_preds = model.predict(x_train)
train_preds

train_targets

"""#Accuracy"""

from sklearn.metrics import accuracy_score
accuracy_score(train_targets,train_preds)

"""We can use a probabilistic prediction using predict_proba"""

train_probs = model.predict_proba(x_train)

model.classes_

from sklearn.metrics import confusion_matrix
confusion_matrix(train_targets,train_preds,normalize = "true")

"""true negative //////                false positive/////
false negative  //////              true positive
"""

def plot_predict (inputs,targets):
  preds = model.predict(inputs)
  accuracy = accuracy_score(targets,preds)
  print(f'the accuracy for the given prediction is : {accuracy*100}')
  cf = confusion_matrix(targets,preds,normalize="true")
  sns.heatmap(cf,annot=True)
  plt.xlabel("Prediction")
  plt.ylabel("Target")
  return preds

inputs = x_train
targets = train_targets
train_preds = plot_predict(x_train,train_targets)

"""#Let's make some predictions !!"""

new_input = { 'Date': '2021-06-19',
'Location': 'Katherine',
'MinTemp': 23.2,
'MaxTemp': 33.2,
'Rainfall': 10.2,
'Evaporation': 4.2,
'Sunshine': np.nan,
'WindGustDir': 'NNW',
'WindGustSpeed': 52.0,
'WindDir9am': 'NW',
'WindDir3pm': 'NNE',
'WindSpeed9am': 13.0,
'WindSpeed3pm': 20.0,
'Humidity9am': 89.0,
'Humidity3pm': 58.0,
'Pressure9am': 1004.8,
'Pressure3pm': 1001.5,
'Cloud9am': 8.0,
'Cloud3pm': 5.0,
'Temp9am': 25.7,
'Temp3pm': 33.0,
'RainToday': 'Yes'}

new_input_df = pd.DataFrame([new_input])

new_input_df

new_input_df[numeric_cols] = imputer.transform(new_input_df[numeric_cols])
new_input_df[numeric_cols] = scaler.transform(new_input_df[numeric_cols])
new_input_df[encoded_cols] = enc.transform(new_input_df[categorical_cols])

x_new_input = new_input_df[numeric_cols + encoded_cols]
x_new_input

prediction = model.predict(x_new_input)[0]
prediction

"""our prediction says that there is rain tomorrow in the location Katherine"""

predict_prob = model.predict_proba(x_new_input)[0]
predict_prob

def predict_input (single_input):
  input_df = pd.DataFrame([single_input])
  input_df [numeric_cols] = imputer.transform(input_df [numeric_cols])
  input_df [ numeric_cols] = scaler.transform(input_df [numeric_cols])
  input_df [ encoded_cols] = enc.transform(input_df [categorical_cols])
  X_input = input_df [ numeric_cols + encoded_cols]
  pred = model.predict(X_input)[0]
  prob = model.predict_proba (X_input) [0] [list (model.classes_).index (pred)]
  return pred, prob

single_input = { 'Date': '2021-06-19',
'Location': 'Katherine',
'MinTemp': 23.2,
'MaxTemp': 33.2,
'Rainfall': 10.2,
'Evaporation': 4.2,
'Sunshine': np.nan,
'WindGustDir': 'NNW',
'WindGustSpeed': 52.0,
'WindDir9am': 'NW',
'WindDir3pm': 'NNE',
'WindSpeed9am': 13.0,
'WindSpeed3pm': 20.0,
'Humidity9am': 89.0,
'Humidity3pm': 58.0,
'Pressure9am': 1004.8,
'Pressure3pm': 1001.5,
'Cloud9am': 8.0,
'Cloud3pm': 5.0,
'Temp9am': 25.7,
'Temp3pm': 33.0,
'RainToday': 'Yes'}

predict_input(single_input)

"""so our model predicted yes for the input with the percentage of 51"""

